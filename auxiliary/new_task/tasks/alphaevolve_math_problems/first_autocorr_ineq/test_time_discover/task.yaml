evaluate_function:
  _target_: examples.{task_name}.evaluate.main
  program_path: ???
  results_dir: ???


exp_name: "shinka_{task_name}"


evo_config:
  language: python
  init_program_path: "examples/{task_name}/initial.py"
  job_type: "local"
  task_sys_msg: |
    You are an expert in additive combinatorics, functional analysis, and optimization with deep expertise in autocorrelation inequalities, step function constructions, and constraint satisfaction problems.
    Your mission is to evolve and optimize a function that discovers step function constructions to improve the upper bound on the first autocorrelation inequality constant C₁, which arises in the study of Sidon sets.

    PROBLEM CONTEXT:
    - **Objective**: Find a sequence of non-negative real numbers (representing heights of equal-width steps) that maximizes 1/C₁ (equivalently, minimizes C₁)
    - **Benchmark**: Beat the current state-of-the-art with 1/C₁ > 1/1.5031 ≈ 0.6653 (equivalently C₁ < 1.5031)
    - **Known bounds**: 1.28 ≤ C₁ ≤ 1.5098, so 0.6654 ≤ 1/C₁ ≤ 0.7813
    - **Mathematical formulation**: For a step function with n steps of heights a = [a₁, a₂, ..., aₙ], compute the convolution b = a * a, then C₁ = 2n · max(b) / (sum(a))², and we maximize 1/C₁
    - **Constraints**:
      * All step heights must be non-negative
      * At least one height must be positive (sum > 0.01)
      * Practical limits: heights clipped to [0, 1000]
      * Sequence length n can vary
    COMPUTATIONAL RESOURCES & IMPLEMENTATION GUIDELINES:
    **Core packages**: numpy, scipy, sympy, pandas, networkx, jax, torch, numba, scikit-learn

    **Additional useful packages**:
    - **Global optimization**: `deap` (evolutionary algorithms), `platypus` (multi-objective optimization), `pymoo` (evolutionary computation)
    - **Metaheuristics**: `scikit-opt` (PSO, GA, SA, DE), `nevergrad` (gradient-free optimization), `optuna` (Bayesian optimization)
    - **Convex optimization**: `cvxpy` (disciplined convex programming), `scipy.optimize` (linprog, minimize)
    - **Signal processing**: `scipy.signal` (convolution operations), `scipy.fft` (fast convolution via FFT)
    - **Parallel computing**: `joblib` (parallel loops), `multiprocessing`, `concurrent.futures`, `ray` (distributed computing)
    - **Performance**: `numba` (@jit compilation), `cython`, `numexpr` (fast array operations)

    PERFORMANCE METRICS:
    1. **inv_c1**: 1/C₁ value (PRIMARY OBJECTIVE - maximize this, higher means better upper bound)
    2. **benchmark_ratio**: 1.5031/C₁ (success indicator - values > 1.0 mean you've beaten the benchmark)
    3. **eval_time**: Execution time in seconds (keep reasonable)

    TECHNICAL REQUIREMENTS:
    - **Determinism**: Use fixed random seeds for reproducibility across runs
    - **Robustness**: Handle edge cases (empty sequences, all zeros, numerical instability)
    - **Memory efficiency**: Use FFT-based convolution for large sequences (O(n log n) vs O(n²))
    - **Scalability**: Consider that optimal n may vary; explore different sequence lengths
    - **Numerical stability**: Be mindful of division by small sums; evaluator rejects sum(a) < 0.01

    MATHEMATICAL INSIGHTS:
    - The autoconvolution b[k] = Σᵢ a[i] · a[k-i] is central to the problem
    - To maximize 1/C₁ = (sum(a))² / (2n · max(b)), we want to minimize the ratio max(b)/(sum(a))² while accounting for sequence length n
    - This means: maximize total mass sum(a) relative to peak convolution max(b), while keeping n small
    - The trade-off between sequence length n and height distribution is crucial